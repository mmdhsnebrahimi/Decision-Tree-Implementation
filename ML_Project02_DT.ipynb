{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project 02\n",
    "## C4.5 Decision Tree Implementation\n",
    "### Iris Dataset Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris  # Only for loading data\n",
    "from sklearn.model_selection import train_test_split  # Only for splitting data\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Node Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self):\n",
    "        self.feature = None\n",
    "        self.threshold = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.class_label = None\n",
    "        self.is_leaf = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: DecisionTreeC4.5 Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeC45:\n",
    "    \n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - max_depth: Maximum depth of the tree (None = no limit)\n",
    "        - min_samples_split: Minimum number of samples required to split a node\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "        self.n_classes = None\n",
    "        self.n_features = None\n",
    "    \n",
    "    \n",
    "    def entropy(self, y):\n",
    "        # Count occurrences of each class\n",
    "        unique_classes, counts = np.unique(y, return_counts=True)\n",
    "        \n",
    "        # Calculate probability of each class\n",
    "        probabilities = counts / len(y)\n",
    "        \n",
    "        # Calculate entropy\n",
    "        entropy_value = 0\n",
    "        for p in probabilities:\n",
    "            if p > 0:  # Avoid log(0)\n",
    "                entropy_value -= p * math.log2(p)\n",
    "        \n",
    "        return entropy_value\n",
    "    \n",
    "    \n",
    "    def information_gain_ratio(self, parent, left_child, right_child):\n",
    "        \"\"\"\n",
    "        Calculate Information Gain Ratio (C4.5 improvement)\n",
    "        \n",
    "        Formulas:\n",
    "            IG = Entropy(parent) - Weighted_Avg(Entropy(children))\n",
    "            SI = Split Information\n",
    "            IGR = IG / SI\n",
    "        \"\"\"\n",
    "        # Calculate parent entropy\n",
    "        entropy_parent = self.entropy(parent)\n",
    "        \n",
    "        # Number of samples\n",
    "        n_total = len(parent)\n",
    "        n_left = len(left_child)\n",
    "        n_right = len(right_child)\n",
    "        \n",
    "        # Calculate children entropy\n",
    "        entropy_left = self.entropy(left_child) if n_left > 0 else 0\n",
    "        entropy_right = self.entropy(right_child) if n_right > 0 else 0\n",
    "        \n",
    "        # Calculate weighted average of children entropy\n",
    "        entropy_weighted = (n_left / n_total) * entropy_left + (n_right / n_total) * entropy_right\n",
    "        \n",
    "        # Information Gain\n",
    "        information_gain = entropy_parent - entropy_weighted\n",
    "        \n",
    "        # Split Information calculation\n",
    "        p_left = n_left / n_total\n",
    "        p_right = n_right / n_total\n",
    "        \n",
    "        split_information = 0\n",
    "        if p_left > 0:\n",
    "            split_information -= p_left * math.log2(p_left)\n",
    "        if p_right > 0:\n",
    "            split_information -= p_right * math.log2(p_right)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        if split_information == 0:\n",
    "            return information_gain\n",
    "        \n",
    "        # Information Gain Ratio\n",
    "        gain_ratio = information_gain / split_information\n",
    "        \n",
    "        return gain_ratio\n",
    "    \n",
    "    \n",
    "    def find_best_split(self, X, y):\n",
    "\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        best_gain_ratio = 0\n",
    "        \n",
    "        # Iterate through all features\n",
    "        for feature_idx in range(X.shape[1]):\n",
    "            feature_values = X[:, feature_idx]\n",
    "            unique_values = np.unique(feature_values)\n",
    "            \n",
    "            # Try all possible thresholds\n",
    "            for i in range(len(unique_values) - 1):\n",
    "                # Threshold is the average of two consecutive values\n",
    "                threshold = (unique_values[i] + unique_values[i + 1]) / 2\n",
    "                \n",
    "                # Split data based on this threshold\n",
    "                left_mask = feature_values <= threshold\n",
    "                right_mask = feature_values > threshold\n",
    "                \n",
    "                # Skip if split is invalid (one side is empty)\n",
    "                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate Information Gain Ratio\n",
    "                gain_ratio = self.information_gain_ratio(y, y[left_mask], y[right_mask])\n",
    "                \n",
    "                # Update best split if this is better\n",
    "                if gain_ratio > best_gain_ratio:\n",
    "                    best_gain_ratio = gain_ratio\n",
    "                    best_feature = feature_idx\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        return best_feature, best_threshold, best_gain_ratio\n",
    "    \n",
    "    \n",
    "    def build_tree(self, X, y, depth=0):\n",
    "\n",
    "        n_samples = len(y)\n",
    "        n_classes = len(np.unique(y))\n",
    "        \n",
    "        # Stopping criterion 1: Pure (one class)\n",
    "        if n_classes == 1:\n",
    "            leaf = Node()\n",
    "            leaf.class_label = y[0]\n",
    "            leaf.is_leaf = True\n",
    "            return leaf\n",
    "        \n",
    "        # Stopping criterion 2: Too few samples\n",
    "        if n_samples < self.min_samples_split:\n",
    "            leaf = Node()\n",
    "            leaf.class_label = np.bincount(y).argmax()  # Majority class\n",
    "            leaf.is_leaf = True\n",
    "            return leaf\n",
    "        \n",
    "        # Stopping criterion 3: Maximum depth reached\n",
    "        if self.max_depth is not None and depth >= self.max_depth:\n",
    "            leaf = Node()\n",
    "            leaf.class_label = np.bincount(y).argmax()\n",
    "            leaf.is_leaf = True\n",
    "            return leaf\n",
    "        \n",
    "        # Find the best split\n",
    "        feature, threshold, gain_ratio = self.find_best_split(X, y)\n",
    "        \n",
    "        # Stopping criterion 4: No good split\n",
    "        if feature is None or gain_ratio == 0:\n",
    "            leaf = Node()\n",
    "            leaf.class_label = np.bincount(y).argmax()\n",
    "            leaf.is_leaf = True\n",
    "            return leaf\n",
    "        \n",
    "        # Create decision node\n",
    "        node = Node()\n",
    "        node.feature = feature\n",
    "        node.threshold = threshold\n",
    "        \n",
    "        # Split data\n",
    "        left_mask = X[:, feature] <= threshold\n",
    "        right_mask = X[:, feature] > threshold\n",
    "        \n",
    "        # Recursively build children\n",
    "        node.left = self.build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        node.right = self.build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the decision tree on training data\n",
    "        \"\"\"\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        self.n_features = X.shape[1]\n",
    "        self.root = self.build_tree(X, y)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def predict_single(self, x, node=None):\n",
    "        \"\"\"\n",
    "        Predict class for a single sample\n",
    "        \"\"\"\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "        \n",
    "        # If leaf node, return class label\n",
    "        if node.is_leaf:\n",
    "            return node.class_label\n",
    "        \n",
    "        # Traverse left or right based on threshold\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self.predict_single(x, node.left)\n",
    "        else:\n",
    "            return self.predict_single(x, node.right)\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict classes for multiple samples\n",
    "        \"\"\"\n",
    "        predictions = np.array([self.predict_single(x) for x in X])\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Dataset Overview\n",
      "\n",
      "Number of samples: 150\n",
      "Number of features: 4\n",
      "Number of classes: 3\n",
      "Class names: ['setosa' 'versicolor' 'virginica']\n",
      "\n",
      "Feature names:\n",
      "  1. sepal length (cm)\n",
      "  2. sepal width (cm)\n",
      "  3. petal length (cm)\n",
      "  4. petal width (cm)\n"
     ]
    }
   ],
   "source": [
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print(\"Iris Dataset Overview\")\n",
    "print(f\"\\nNumber of samples: {len(X)}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"Number of classes: {len(iris.target_names)}\")\n",
    "print(f\"Class names: {iris.target_names}\")\n",
    "print(f\"\\nFeature names:\")\n",
    "for i, name in enumerate(iris.feature_names, 1):\n",
    "    print(f\"  {i}. {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Split Data into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data split:\n",
      "  Train: 120 samples\n",
      "  Test:  30 samples\n"
     ]
    }
   ],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"\\nData split:\")\n",
    "print(f\"  Train: {len(X_train)} samples\")\n",
    "print(f\"  Test:  {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create and train the model\n",
    "model = DecisionTreeC45(max_depth=10, min_samples_split=2)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Make Predictions and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Results\n",
      "\n",
      "Train Accuracy: 1.0000 (100.00%)\n",
      "Test Accuracy:  0.9333 (93.33%)\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_train = np.mean(y_pred_train == y_train)\n",
    "accuracy_test = np.mean(y_pred_test == y_test)\n",
    "\n",
    "print(\"Prediction Results\")\n",
    "print(f\"\\nTrain Accuracy: {accuracy_train:.4f} ({accuracy_train*100:.2f}%)\")\n",
    "print(f\"Test Accuracy:  {accuracy_test:.4f} ({accuracy_test*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Detailed Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Evaluation Metrics (Test Set)\n",
      "\n",
      "Class: setosa\n",
      "   Precision: 1.0000\n",
      "   Recall:    1.0000\n",
      "   F1-Score:  1.0000\n",
      "\n",
      "Class: versicolor\n",
      "   Precision: 0.9000\n",
      "   Recall:    0.9000\n",
      "   F1-Score:  0.9000\n",
      "\n",
      "Class: virginica\n",
      "   Precision: 0.9000\n",
      "   Recall:    0.9000\n",
      "   F1-Score:  0.9000\n",
      "\n",
      "Average Metrics (Macro Average):\n",
      "   Precision: 0.9333\n",
      "   Recall:    0.9333\n",
      "   F1-Score:  0.9333\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(y_true, y_pred, n_classes, class_names):\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics for each class\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    for class_idx in range(n_classes):\n",
    "        # True Positives, False Positives, False Negatives, True Negatives\n",
    "        tp = np.sum((y_pred == class_idx) & (y_true == class_idx))\n",
    "        fp = np.sum((y_pred == class_idx) & (y_true != class_idx))\n",
    "        fn = np.sum((y_pred != class_idx) & (y_true == class_idx))\n",
    "        tn = np.sum((y_pred != class_idx) & (y_true != class_idx))\n",
    "        \n",
    "        # Calculate metrics\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        metrics[class_names[class_idx]] = {\n",
    "            'TP': tp, 'FP': fp, 'FN': fn, 'TN': tn,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate metrics\n",
    "metrics_test = calculate_metrics(y_test, y_pred_test, len(iris.target_names), iris.target_names)\n",
    "\n",
    "print(\"Detailed Evaluation Metrics (Test Set)\")\n",
    "\n",
    "for class_name in iris.target_names:\n",
    "    m = metrics_test[class_name]\n",
    "    print(f\"\\nClass: {class_name}\")\n",
    "    print(f\"   Precision: {m['Precision']:.4f}\")\n",
    "    print(f\"   Recall:    {m['Recall']:.4f}\")\n",
    "    print(f\"   F1-Score:  {m['F1-Score']:.4f}\")\n",
    "\n",
    "# Average metrics\n",
    "avg_precision = np.mean([metrics_test[cn]['Precision'] for cn in iris.target_names])\n",
    "avg_recall = np.mean([metrics_test[cn]['Recall'] for cn in iris.target_names])\n",
    "avg_f1 = np.mean([metrics_test[cn]['F1-Score'] for cn in iris.target_names])\n",
    "\n",
    "print(\"\\nAverage Metrics (Macro Average):\")\n",
    "print(f\"   Precision: {avg_precision:.4f}\")\n",
    "print(f\"   Recall:    {avg_recall:.4f}\")\n",
    "print(f\"   F1-Score:  {avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Test Set)\n",
      "\n",
      "                           Predicted\n",
      "               setosa      versicolor  virginica   \n",
      "------------------------------------------------------------\n",
      "Actual setosa    10          0           0           \n",
      "Actual versicolor0           9           1           \n",
      "Actual virginica 0           1           9           \n"
     ]
    }
   ],
   "source": [
    "def confusion_matrix(y_true, y_pred, n_classes):\n",
    "    \"\"\"\n",
    "    Calculate Confusion Matrix\n",
    "    \"\"\"\n",
    "    cm = np.zeros((n_classes, n_classes), dtype=int)\n",
    "    \n",
    "    for i in range(len(y_true)):\n",
    "        actual = y_true[i]\n",
    "        predicted = y_pred[i]\n",
    "        cm[actual][predicted] += 1\n",
    "    \n",
    "    return cm\n",
    "\n",
    "# Calculate Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_test, len(iris.target_names))\n",
    "\n",
    "print(\"Confusion Matrix (Test Set)\")\n",
    "print(\"\\n\" + \" \"*27 + \"Predicted\")\n",
    "header = \" \"*15\n",
    "for name in iris.target_names:\n",
    "    header += f\"{name:<12}\"\n",
    "print(header)\n",
    "print(\"-\"*60)\n",
    "\n",
    "for i, class_name in enumerate(iris.target_names):\n",
    "    row = f\"Actual {class_name:<10}\"\n",
    "    for j in range(len(iris.target_names)):\n",
    "        row += f\"{cm[i, j]:<12}\"\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
